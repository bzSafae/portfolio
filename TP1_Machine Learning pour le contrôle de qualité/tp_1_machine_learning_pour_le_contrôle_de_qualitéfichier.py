# -*- coding: utf-8 -*-
"""TP 1 : Machine Learning pour le contrôle de qualitéFichier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XTK67tJf6iMVjQHJhVuWl-Kgl8H86bRk
"""

import pandas as pd

# Charger les données
file_path = "/content/uci-secom.csv"
df = pd.read_csv(file_path)

import pandas as pd

# Charger les données
file_path = "/content/uci-secom.csv"
df = pd.read_csv(file_path)

# 1. Afficher les dimensions du dataframe
print("Dimensions du dataframe:", df.shape)

# 2. Afficher les 10 premières lignes
print("\nPremières lignes du dataframe:")
print(df.head(10))

# 3. Utiliser la méthode info() pour afficher les détails des colonnes
print("\nInformations sur le dataframe:")
print(df.info())

# 4. Utiliser la méthode describe() pour afficher les statistiques
print("\nStatistiques descriptives du dataframe:")
print(df.describe())

#Partie 2 Analyse des donnée :

# 1. Vérifier les types de données des colonnes
print("\nTypes des données des colonnes:")
print(df.dtypes)

# Détection des valeurs manquantes
print("="*40)
print(" Détection des valeurs manquantes ")
print("="*40)

missing_values = df.isnull().sum()
missing_columns = missing_values[missing_values > 0]

print(f"Nombre total de colonnes avec des valeurs manquantes: {len(missing_columns)}")
print("\nListe des colonnes avec valeurs manquantes et leur nombre:")
print(missing_columns)
print("="*40)



# Définir un seuil pour supprimer les colonnes trop incomplètes (ex: 50% de valeurs manquantes)
missing_threshold = 0.3
missing_ratio = df.isnull().sum() / len(df)

# Colonnes à supprimer
columns_to_drop = missing_ratio[missing_ratio > missing_threshold].index
df_cleaned = df.drop(columns=columns_to_drop)

print("="*40)
print(" Suppression des colonnes avec plus de 50% de valeurs manquantes ")
print("="*40)
print(f"Nombre de colonnes supprimées: {len(columns_to_drop)}")
print(f"Nouvelle taille du dataframe: {df_cleaned.shape}")
print("="*40)

# Définir le chemin du fichier de sortie
output_file = "/content/secom_cleaned.csv"

# Sauvegarder le dataframe nettoyé
df_cleaned.to_csv(output_file, index=False)

print("="*40)
print(f" Fichier enregistré sous : {output_file} ")
print("="*40)

'''# Supprimer les colonnes contenant uniquement des zéros
df_cleaned = df_cleaned.loc[:, (df_cleaned != 0).any(axis=0)]

# Sauvegarder la base de données
df_cleaned.to_csv("cleaned_data.csv", index=False)

# Afficher le nombre de colonnes restantes
print(f"Nombre de colonnes restantes : {df_cleaned.shape[1]}")'''

# Définir le seuil de 30 %
threshold = 0.3

# Identifier les colonnes qui contiennent plus de 30 % de zéros exacts (sauf la colonne cible)
zero_columns = [col for col in df_cleaned.columns[:-1]  # Exclure la colonne cible
               if (df_cleaned[col] == 0).mean() > threshold]  # Vérifier si plus de 30 % de zéros

# Afficher les colonnes à supprimer
print("="*40)
print(" Colonnes à supprimer (plus de 30 % de zéros exacts) ")
print("="*40)
print(zero_columns)

# Supprimer ces colonnes
df_cleaned = df_cleaned.drop(columns=zero_columns)

# Afficher la nouvelle taille du dataframe
print("="*40)
print(" Suppression des colonnes contenant plus de 30 % de zéros exacts ")
print("="*40)
print(f"Nombre de colonnes supprimées: {len(zero_columns)}")
print(f"Nouvelle taille du dataframe: {df_cleaned.shape}")
print("="*40)
# Sauvegarder la base de données
df_cleaned.to_csv("cleaned_data.csv", index=False)

# Définir le seuil de 30 %
threshold = 0.5

# Identifier les colonnes avec plus de 30 % de valeurs manquantes
missing_ratio = df_cleaned.isna().mean()  # Proportion de NaN par colonne
columns_to_drop = missing_ratio[missing_ratio > threshold].index

# Afficher les colonnes à supprimer
print("="*40)
print(" Colonnes à supprimer (plus de 30 % de valeurs manquantes) ")
print("="*40)
print(columns_to_drop)

# Supprimer ces colonnes
df_cleaned2 = df_cleaned.drop(columns=columns_to_drop)

# Afficher la nouvelle taille du dataframe
print("="*40)
print(" Suppression des colonnes avec plus de 30 % de valeurs manquantes ")
print("="*40)
print(f"Nombre de colonnes supprimées: {len(columns_to_drop)}")
print(f"Nouvelle taille du dataframe: {df_cleaned.shape}")
print("="*40)
# Sauvegarder la base de données
df_cleaned2.to_csv("cleaned_data2.csv", index=False)

# Supprimer la première colonne
df_cleaned2 = df_cleaned2.drop(df_cleaned.columns[0], axis=1)

# Afficher les premières lignes pour vérifier
print("="*40)
print(" Suppression de la première colonne ")
print("="*40)
print(df_cleaned.head())
print("="*40)

# Remplacer les valeurs manquantes par la moyenne des colonnes numériques
df_cleaned2 = df_cleaned.fillna(df_cleaned.select_dtypes(include=['number']).mean())

# Afficher le nombre total de valeurs manquantes après traitement
print("="*40)
print(" Remplacement des valeurs manquantes par la moyenne ")
print("="*40)
print(f"Nombre total de valeurs manquantes après traitement: {df_cleaned.isnull().sum().sum()}")
print("="*40)
# Sauvegarder la base de données
df_cleaned2.to_csv("cleaned_data3.csv", index=False)

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Charger les données (assurez-vous que le chemin est correct)
file_path = "/content/cleaned_data3.csv"
df = pd.read_csv(file_path)

# Encoder la colonne "Phase" avec Label Encoding
le = LabelEncoder()
df['Phase'] = le.fit_transform(df['Phase'])

# Sauvegarder la base de données avec la colonne "Phase" encodée
df.to_csv("cleaned_data_encoded.csv", index=False)

print("La colonne 'Phase' a été encodée avec Label Encoding et la base de données a été sauvegardée sous cleaned_data_encoded.csv")

import pandas as pd
# Charger les données (assurez-vous que le chemin est correct)
file_path = "/content/cleaned_data_encoded.csv"
df = pd.read_csv(file_path)

# Vérifier les valeurs uniques dans la colonne "Phase" (cible)
unique_target_values = df['Phase'].unique()
print("Valeurs uniques de la colonne 'Phase':", unique_target_values)

# Compter le nombre de Pass et Fail
pass_count = (df['Phase'] == 1).sum()
fail_count = (df['Phase'] == 0).sum()

print("Nombre de 'Pass':", pass_count)
print("Nombre de 'Fail':", fail_count)

# Calculer le ratio Pass/Fail
if fail_count > 0:
  ratio_pass_fail = pass_count / fail_count
  print("Ratio Pass/Fail:", ratio_pass_fail)
else:
  print("Aucun 'Fail' trouvé dans la colonne 'Phase'.")

import pandas as pd
# Charger les données (assurez-vous que le chemin est correct)
file_path = "/content/cleaned_data_encoded.csv"
df = pd.read_csv(file_path)

# Compter le nombre de Pass et Fail
pass_count = (df['Phase'] == 1).sum()
fail_count = (df['Phase'] == 0).sum()

print("Nombre de 'Pass':", pass_count)
print("Nombre de 'Fail':", fail_count)

# Déterminer le nombre d'échantillons à supprimer de la classe 'Fail'
if fail_count > pass_count:
  samples_to_remove = fail_count - pass_count

  # Obtenir un échantillon aléatoire d'indices de la classe 'Fail'
  fail_indices = df[df['Phase'] == 0].index
  indices_to_remove = fail_indices[:samples_to_remove]

  # Supprimer les échantillons de la classe 'Fail'
  df = df.drop(indices_to_remove)

# Vérifier la distribution après la suppression
class_counts = df['Phase'].value_counts()
print("\nDistribution de la classe 'Phase' après suppression:")
print(class_counts)

# Sauvegarder le DataFrame avec les données équilibrées
df.to_csv("balanced_data.csv", index=False)

print("\nDonnées équilibrées sauvegardées dans 'balanced_data.csv'")

# prompt: maintennat on va prendre la derniere base de donne  : balanced_data.csv
# et on va faire le redimensionnement par la corrélation et la variance
# fait une etude de corrélation et les features qui sont les plus collorée on v supprimer l'une et laisser l'autre

import pandas as pd

# Charger les données équilibrées
file_path = "/content/balanced_data.csv"
df = pd.read_csv(file_path)

# Calculer la matrice de corrélation
correlation_matrix = df.corr()

# Identifier les paires de caractéristiques fortement corrélées
threshold = 0.8  # Seuil de corrélation
correlated_features = set()
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            colname_i = correlation_matrix.columns[i]
            colname_j = correlation_matrix.columns[j]
            correlated_features.add((colname_i, colname_j))

# Afficher les caractéristiques fortement corrélées
print("Caractéristiques fortement corrélées (seuil de corrélation > 0.8):")
for feature_pair in correlated_features:
    print(feature_pair)


# Sélectionner une caractéristique de chaque paire pour supprimer
features_to_drop = []
for feature_pair in correlated_features:
  # Sélectionner la caractéristique avec la variance la plus faible
    variance_i = df[feature_pair[0]].var()
    variance_j = df[feature_pair[1]].var()
    if variance_i < variance_j:
        features_to_drop.append(feature_pair[0])
    else:
        features_to_drop.append(feature_pair[1])


# Supprimer les caractéristiques sélectionnées
df_reduced = df.drop(columns=features_to_drop)

# Afficher la nouvelle taille du DataFrame
print("\nTaille du DataFrame après réduction des caractéristiques:", df_reduced.shape)

# Sauvegarder le DataFrame avec les caractéristiques réduites
df_reduced.to_csv("reduced_data.csv", index=False)
print("DataFrame avec caractéristiques réduites sauvegardé dans 'reduced_data.csv'")

import pandas as pd

# Charger la dernière base de données
file_path = "/content/reduced_data.csv"
df = pd.read_csv(file_path)

# Séparer les features (X) et la cible (Y)
X = df.drop(columns=['Pass/Fail'])  # Features (toutes les colonnes sauf 'Pass/Fail')
Y = df['Pass/Fail']                 # Cible (colonne 'Pass/Fail')

# Vérifier la taille des vecteurs
print("="*40)
print(" Vérification des tailles des vecteurs ")
print("="*40)
print(f"Taille de X (features): {X.shape}")
print(f"Taille de Y (target): {Y.shape}")
print("="*40)

# Afficher les premières lignes pour vérifier
print("Premières lignes de X (features):")
print(X.head())
print("\nPremières lignes de Y (target):")
print(Y.head())

from sklearn.preprocessing import MinMaxScaler

# Initialiser MinMaxScaler
scaler = MinMaxScaler()

# Appliquer la standardisation aux features (X)
X_scaled = scaler.fit_transform(X)

# Convertir le résultat en dataframe pour une meilleure visualisation
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# Afficher les premières lignes des données standardisées
print("="*40)
print(" Données standardisées avec MinMaxScaler ")
print("="*40)
print(X_scaled.head())
print("="*40)

# Vérifier les valeurs minimales et maximales après standardisation
print("Valeurs minimales après standardisation :")
print(X_scaled.min())
print("\nValeurs maximales après standardisation :")
print(X_scaled.max())